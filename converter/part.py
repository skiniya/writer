import torch
from pathlib import Path

def convert(sec):
    sec = sec % (24 * 3600)
    hour = sec // 3600
    sec %= 3600
    min = sec // 60
    sec %= 60
    return "%02d:%02d:%02d" % (hour, min, sec)
def time_to_sec(t1):
        h, m, s = map(int, t1.split(':'))
        return h * 3600 + m * 60 + s
        print("\n–í–≤–µ–¥–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ hh:mm:ss")
        t1 = input()
        t11 : int = time_to_sec(t1)
def time_to_sec(t2):
        h, m, s = map(int, t2.split(':'))
        return h * 3600 + m * 60 + s
        print("\n–í–≤–µ–¥–∏—Ç–µ –∫–æ–Ω–µ—á–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ hh:mm:ss")
        t2 = input()
        t22 : int = time_to_sec(t2)

def partAudio():
    from faster_whisper import WhisperModel
    from tqdm import tqdm
    from pydub import AudioSegment
    import fnmatch
    import glob, os

    print("–ù–∞—á–∏–Ω–∞—é —Ä–∞–±–æ—Ç—É —Å –∞—É–¥–∏–æ:")

    os.chdir("./")
    for file in glob.glob("*.mp3"):
        print(file)
        print("–í–≤–µ–¥–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ hh:mm:ss")
        t1 = input()
        t11: int = time_to_sec(t1)
        print("–í–≤–µ–¥–∏—Ç–µ –∫–æ–Ω–µ—á–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ hh:mm:ss")
        t2 = input()
        t22: int = time_to_sec(t2)
        print("–ó–∞–ø—É—Å–∫–∞—é –ø—Ä–æ—Ü–µ—Å—Å...")

        t_start = t11 * 1000  # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç –Ω–∞—á–∞–ª–∞ - 0 * 1000
        t_end = t22 * 1000  # 60 —Å–µ–∫—É–Ω–¥ - –¥–æ 1-–π –º–∏–Ω—É—Ç—ã
        new_audio = AudioSegment.from_mp3(file)
        a = new_audio[t_start:t_end]
        a.export("fragment.mp3", format="mp3")
        dst = "audio.wav"
        sound = AudioSegment.from_mp3("fragment.mp3")
        sound.set_channels(1)
        sound = sound.set_frame_rate(16000)
        sound = sound.set_channels(1)
        sound.export(dst, format="wav")

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if torch.cuda.is_available():
            device = torch.device('cuda')
            model = WhisperModel("large-v2", device="cuda", compute_type="int8_float16")
            print("\n–ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ GPU: large-v2.\n")
        else:
            device = torch.device('cpu')
            model = WhisperModel("medium", device="cpu", compute_type="int8")
            print("\n–ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ CPU: medium.\n")

        segments, info = model.transcribe("audio.wav", beam_size=1)
        print("–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: %s (%d—Å) " % (convert(info.duration), info.duration))
        print('\n–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç...')

        myfile = open("text.txt", "a")
        for segment in tqdm(segments, unit_scale=100, unit=' —Ü–∏–∫–ª–æ–≤', ascii=False, ncols=100, position=0,
                            total=(int(info.duration / 4)), dynamic_ncols=100, colour='#9ACD32'):
            myfile.write(segment.text)
            # print("[%s] %s" % (convert(segment.start), segment.text))
        myfile.close()

        Path('text.txt').write_text(Path('text.txt').read_text().replace('—ë', '–µ'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–ò—Å—É—Å', '–ò–∏—Å—É—Å'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–£–∑—è', '–£–∑–∑–∏—è'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–£—Å—è', '–£–∑–∑–∏—è'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–£–∑—é', '–£–∑–∑–∏—é'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–£–∑–∏—è', '–£–∑–∑–∏—è'))
        Path('text.txt').write_text(Path('text.txt').read_text().replace('–£–Ω–∏—è', '–£–∑–∑–∏—è'))

        print("\n–°–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ –∑–∞–º–µ–Ω–µ–Ω—ã.")
        print("\nüìå –ú–æ–∂–Ω–æ –∑–∞–±–∏—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ txt —Ñ–∞–π–ª–∞.")
    else:
        print("\n–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == '__main__':
    partAudio()
